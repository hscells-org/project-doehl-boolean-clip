{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39557edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simon\\miniconda3\\envs\\RP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer, Siglip2TextModel\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DualSiglip2Model(nn.Module):\n",
    "    def __init__(self, model_name=\"google/siglip2-base-patch16-224\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.encoder_bool = Siglip2TextModel.from_pretrained(model_name)\n",
    "        self.encoder_text = deepcopy(self.encoder_bool)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.to(device)\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        return self.tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=64).to(device)\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip2/modeling_siglip2.py#L952\n",
    "    def forward(self, in_bool, in_text):\n",
    "        tok_bool = self.tokenize(in_bool)\n",
    "        tok_text = self.tokenize(in_text)\n",
    "        out_bool = self.encoder_bool(**tok_bool).pooler_output\n",
    "        out_text = self.encoder_text(**tok_text).pooler_output\n",
    "        out_bool = out_bool / out_bool.norm(p=2, dim=-1, keepdim=True)\n",
    "        out_text = out_text / out_text.norm(p=2, dim=-1, keepdim=True)\n",
    "        loss = self.loss(out_bool, out_text)\n",
    "        logits = out_bool @ out_text.t() + self.bias\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def loss(self, emb_a, emb_b):\n",
    "        sim = emb_a @ emb_b.t() + self.bias\n",
    "        eye = torch.eye(sim.size(0), device=sim.device)\n",
    "        y = -torch.ones_like(sim)\n",
    "        y = y + 2 * eye\n",
    "        loglik = F.logsigmoid(y * sim)\n",
    "        nll = -torch.sum(loglik, dim=-1)\n",
    "        loss = nll.mean()\n",
    "        return loss\n",
    "\n",
    "    def load(self, path):\n",
    "        state_dict = load_file(path, device)\n",
    "        self.load_state_dict(state_dict, strict=False)\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, in_bool, in_text):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(in_bool, in_text)\n",
    "            logits = outputs[\"logits\"]\n",
    "            probs = torch.sigmoid(logits)\n",
    "        return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facba3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import Dataset, IterableDataset\n",
    "import numpy as np\n",
    "\n",
    "class RandomAccessMismatchedPairs:\n",
    "    def __init__(self, dataset, key_a='q', key_b='d'):\n",
    "        self.dataset = dataset\n",
    "        self.key_a = key_a\n",
    "        self.key_b = key_b\n",
    "        self.size = len(dataset)\n",
    "        self.total_pairs = self.size * (self.size - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_pairs\n",
    "\n",
    "    def _index_to_pair(self, index):\n",
    "        i = index // (self.size - 1)\n",
    "        j = index % (self.size - 1)\n",
    "        if j >= i: j += 1\n",
    "        return i, j\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < 0 or index >= self.total_pairs:\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        i, j = self._index_to_pair(index)\n",
    "        return {\n",
    "            self.key_a: self.dataset[i][self.key_a],\n",
    "            self.key_b: self.dataset[j][self.key_b],\n",
    "        }\n",
    "\n",
    "    def random_sample(self, k=1, seed=42):\n",
    "        rng = random.Random(seed)\n",
    "        indices = rng.sample(range(self.total_pairs), k)\n",
    "        return [self[i] for i in indices]\n",
    "\n",
    "    def get_n_pairs(self, n, random_order=False, seed=42):\n",
    "        if n > self.total_pairs:\n",
    "            raise ValueError(\"Requested more pairs than available.\")\n",
    "        if random_order:\n",
    "            return self.random_sample(n, seed)\n",
    "        else:\n",
    "            return [self[i] for i in range(n)]\n",
    "\n",
    "    def to_dataset(self, n=None, random_order=False, seed=42):\n",
    "        if n is None:\n",
    "            n = self.total_pairs\n",
    "        data = self.get_n_pairs(n, random_order, seed)\n",
    "        return Dataset.from_list(data)\n",
    "\n",
    "def create_mismatched_dataset(dataset: Dataset) -> Dataset:\n",
    "    qs = np.array(dataset[\"q\"])\n",
    "    ds = np.array(dataset[\"d\"])\n",
    "\n",
    "    n = len(qs)\n",
    "    idx_q = np.repeat(np.arange(n), n - 1)\n",
    "    idx_d = np.array([np.delete(np.arange(n), i) for i in range(n)]).flatten()\n",
    "\n",
    "    mismatched_qs = qs[idx_q]\n",
    "    mismatched_ds = ds[idx_d]\n",
    "\n",
    "    return Dataset.from_dict({\"q\": mismatched_qs.tolist(), \"d\": mismatched_ds.tolist()})\n",
    "\n",
    "def mismatched_generator(dataset):\n",
    "    data = list(dataset)\n",
    "    for i, item_q in enumerate(data):\n",
    "        for j, item_d in enumerate(data):\n",
    "            if i != j:\n",
    "                yield {\"q\": item_q[\"q\"], \"d\": item_d[\"d\"]}\n",
    "\n",
    "def create_lazy_mismatched_dataset(dataset: Dataset) -> IterableDataset:\n",
    "    return IterableDataset.from_generator(lambda: mismatched_generator(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a491c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['q', 'd'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['q', 'd'],\n",
      "        num_rows: 6100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['q', 'd'],\n",
      "        num_rows: 1525\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type siglip_text_model to instantiate a model of type siglip2_text_model. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"data\", split=\"train\")\n",
    "\n",
    "neg_data = RandomAccessMismatchedPairs(dataset, key_a=\"q\", key_b=\"d\").to_dataset(1000, random_order=True)\n",
    "print(neg_data)\n",
    "\n",
    "data = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(data)\n",
    "\n",
    "model = DualSiglip2Model()\n",
    "\n",
    "# def preprocess(example):\n",
    "#     t1 = model.tokenize(example[\"q\"])\n",
    "#     t2 = model.tokenize(example[\"d\"])\n",
    "#     return {\"q\": t1, \"d\": t2}\n",
    "\n",
    "# data = data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ac7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Untrained [[0.6791712  0.66508985 0.6837426  ... 0.67505753 0.68886775 0.6758862 ]\n",
      " [0.6769943  0.6663372  0.6767399  ... 0.6728213  0.6815574  0.6702494 ]\n",
      " [0.6878812  0.67477584 0.6922052  ... 0.6883122  0.6967495  0.6881189 ]\n",
      " ...\n",
      " [0.6876252  0.67707604 0.69353455 ... 0.68361294 0.69489396 0.68127614]\n",
      " [0.68463874 0.6735706  0.68839514 ... 0.6820059  0.6871852  0.67682475]\n",
      " [0.6744925  0.6638663  0.6794575  ... 0.6737547  0.68450654 0.6801185 ]]\n",
      "Negative Untrained [[0.68646175 0.6822819  0.6894692  ... 0.6853466  0.68375593 0.7064092 ]\n",
      " [0.68411094 0.68023825 0.68619466 ... 0.6847603  0.6834683  0.695462  ]\n",
      " [0.68516934 0.6828417  0.68859494 ... 0.68543124 0.6832921  0.7063381 ]\n",
      " ...\n",
      " [0.7043689  0.705027   0.70461035 ... 0.70472145 0.7098322  0.7035266 ]\n",
      " [0.69072527 0.686063   0.6929386  ... 0.68947005 0.6872002  0.70067495]\n",
      " [0.6893828  0.6842517  0.6929224  ... 0.6896274  0.68589926 0.7108423 ]]\n",
      "Positive Trained [[0.7640605  0.7523464  0.76783586 ... 0.7606517  0.7720525  0.7613393 ]\n",
      " [0.762258   0.7533893  0.76204705 ... 0.75879407 0.7660328  0.75665355]\n",
      " [0.77124214 0.76041794 0.77478945 ... 0.77159625 0.7785046  0.77143747]\n",
      " ...\n",
      " [0.77103174 0.76232576 0.77587754 ... 0.76772887 0.77698916 0.7658006 ]\n",
      " [0.76857436 0.7594168  0.77166444 ... 0.76640314 0.77067006 0.7621174 ]\n",
      " [0.7601827  0.75132245 0.7642973  ... 0.7595698  0.76846546 0.76484394]]\n",
      "Negative Trained [[0.77007514 0.76663095 0.77254635 ... 0.7691573  0.7678468  0.78635854]\n",
      " [0.7681394  0.7649429  0.7698554  ... 0.7686745  0.7676097  0.77745336]\n",
      " [0.7690113  0.7670929  0.7718285  ... 0.7692271  0.76746434 0.78630096]\n",
      " ...\n",
      " [0.78470457 0.7852384  0.78490037 ... 0.7849905  0.7891277  0.78402096]\n",
      " [0.7735767  0.76974696 0.77538985 ... 0.772547   0.7706824  0.7817033 ]\n",
      " [0.7724754  0.76825553 0.7753766  ... 0.77267617 0.76961225 0.7899434 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive Untrained\", model.evaluate(data[\"test\"][\"q\"], data[\"test\"][\"d\"]))\n",
    "print(\"Negative Untrained\", model.evaluate(neg_data[\"q\"], neg_data[\"d\"]))\n",
    "model = model.load(r\"./siglip2/checkpoint-18300/model.safetensors\")\n",
    "print(\"Positive Trained\", model.evaluate(data[\"test\"][\"q\"], data[\"test\"][\"d\"]))\n",
    "print(\"Negative Trained\", model.evaluate(neg_data[\"q\"], neg_data[\"d\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cf2cb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DualSiglip2Model' object has no attribute '_keys_to_ignore_on_save'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     27\u001b[39m trainer = Trainer(\n\u001b[32m     28\u001b[39m     model,\n\u001b[32m     29\u001b[39m     training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     data_collator=collate_fn,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# print(trainer.evaluate())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# print(trainer.evaluate())\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Simon\\miniconda3\\envs\\RP\\Lib\\site-packages\\transformers\\trainer.py:2217\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fsdp_enabled:\n\u001b[32m-> \u001b[39m\u001b[32m2217\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2218\u001b[39m     \u001b[38;5;66;03m# In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\u001b[39;00m\n\u001b[32m   2219\u001b[39m     state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Simon\\miniconda3\\envs\\RP\\Lib\\site-packages\\transformers\\trainer.py:2867\u001b[39m, in \u001b[36mTrainer._load_from_checkpoint\u001b[39m\u001b[34m(self, resume_from_checkpoint, model)\u001b[39m\n\u001b[32m   2865\u001b[39m         \u001b[38;5;66;03m# release memory\u001b[39;00m\n\u001b[32m   2866\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_issue_warnings_after_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2869\u001b[39m \u001b[38;5;66;03m# Load adapters following PR # 24096\u001b[39;00m\n\u001b[32m   2870\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_peft_model(model):\n\u001b[32m   2871\u001b[39m     \u001b[38;5;66;03m# If train a model using PEFT & LoRA, assume that adapter have been saved properly.\u001b[39;00m\n\u001b[32m   2872\u001b[39m     \u001b[38;5;66;03m# TODO: in the future support only specific min PEFT versions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Simon\\miniconda3\\envs\\RP\\Lib\\site-packages\\transformers\\trainer.py:3033\u001b[39m, in \u001b[36mTrainer._issue_warnings_after_load\u001b[39m\u001b[34m(self, load_result)\u001b[39m\n\u001b[32m   3031\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_issue_warnings_after_load\u001b[39m(\u001b[38;5;28mself\u001b[39m, load_result):\n\u001b[32m   3032\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(load_result.missing_keys) != \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3033\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_keys_to_ignore_on_save\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mset\u001b[39m(load_result.missing_keys) == \u001b[38;5;28mset\u001b[39m(\n\u001b[32m   3034\u001b[39m             \u001b[38;5;28mself\u001b[39m.model._keys_to_ignore_on_save\n\u001b[32m   3035\u001b[39m         ):\n\u001b[32m   3036\u001b[39m             \u001b[38;5;28mself\u001b[39m.model.tie_weights()\n\u001b[32m   3037\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Simon\\miniconda3\\envs\\RP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'DualSiglip2Model' object has no attribute '_keys_to_ignore_on_save'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./siglip2\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    # max_steps=1000,\n",
    "    # bf16=True,\n",
    "    # optim=\"adamw_bnb_8bit\",\n",
    "    # torch_compile=True,\n",
    "    # torch_compile_backend=\"inductor\"\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    d = {\n",
    "        \"a\": torch.tensor([ex[\"q\"][\"input_ids\"] for ex in batch]),\n",
    "        \"b\": torch.tensor([ex[\"d\"][\"input_ids\"] for ex in batch]),\n",
    "    }\n",
    "    return d\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# print(trainer.evaluate())\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# print(trainer.evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
